# need 4 gpus
# torchrun --nnodes 1 --nproc_per_node 8 run_explainable_same_word_embedding_cross_attention_debug_llama3_copy.py args/gsm_coconut_stage1_3latent_6tokens.yaml

project: coconut
save_path: /root/autodl-tmp/simcot_ckpts/Coconut/ckpts
name: gsm-coconut

only_eval: False

coconut: True
cot: False
no_thoughts: False
no_cot: False
coconutgpt: True

c_thought: 2
epochs_per_stage: 3
max_latent_stage: 5
pad_latent_to_max: True
# progressive_train: True

save_only_improve: False
uniform_prob: 0.0

# Model and checkpoint paths (use relative paths or env variables)
#model_id: ./pretrained/gpt2
#model_id: /share/xuyinlong/huangxutao/models/llama3.2_1b_instruct/origin_model
model_id: /root/autodl-tmp/models/llama3.2_1b_instruct/origin_model
#load_model_path: ./ckpts/gsm_cot/checkpoint_20
load_model_path:

wandb: True
seed: 0
# resume: 3
resume: 0
# bf16: False
bf16: True

# for debug
# train_path: ./data/gsm_train.json 
# val_path: ./data/gsm_test.json
# train_path: /share/xuyinlong/huangxutao/datasets/gsm8k_aug/train.json
# val_path: /share/xuyinlong/huangxutao/datasets/gsm8k_aug/test.json
train_path: /root/autodl-tmp/datasets/gsm8k_aug/train.json
val_path: /root/autodl-tmp/datasets/gsm8k_aug/test.json


reset_optimizer: True
batch_size_training: 24
debug: False
gradient_accumulation_steps: 5
num_epochs: 25
lr: !!float "1e-4"
weight_decay: 0.01

mode: coconut_baseline
## full, only_base_causallm, only_expainable_llm
training_method: only_base_causallm

