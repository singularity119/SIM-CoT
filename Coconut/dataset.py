# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

import json
import itertools
import random
from dataclasses import dataclass
from typing import Optional

import torch
import torch.distributed as dist
from datasets import Dataset
from transformers import PreTrainedTokenizerBase
from transformers.data.data_collator import pad_without_fast_tokenizer_warning

# === 辅助函数：安全的分布式 Map ===
def safe_map(dataset, func, **kwargs):
    """
    让 Rank 0 先处理并写入缓存，其他 Rank 等待。
    避免多进程同时写缓存导致 SIGABRT 崩溃。
    """
    # 如果是分布式环境，非 0 号卡先等待
    if dist.is_initialized() and dist.get_rank() != 0:
        dist.barrier()

    # 执行 map (Rank 0 会计算写入，其他人后续会直接读缓存)
    mapped_dataset = dataset.map(func, **kwargs)

    # Rank 0 处理完后，解除等待
    if dist.is_initialized() and dist.get_rank() == 0:
        dist.barrier()
        
    return mapped_dataset
# =================================

def get_dataset(path, tokenizer, max_size=1000000000):

    def tokenize_sample(sample):

        question_tokenized = tokenizer.encode(
            sample["question"] + "\n", add_special_tokens=True
        )
        steps_tokenized = [
            tokenizer.encode(s + "\n", add_special_tokens=False)
            for s in sample["steps"]
        ]
        answer_tokenized = tokenizer.encode(
            "### " + sample["answer"], add_special_tokens=False
        ) + [tokenizer.eos_token_id]

        sample = {
            "question_tokenized": question_tokenized,
            "steps_tokenized": steps_tokenized,
            "answer_tokenized": answer_tokenized,
            "idx": sample["idx"],
        }
        return sample

    # 先把JSON读进来
    with open(path, "r") as f:
        raw = json.load(f)

    # 情况1：顶层是 dict，且包含 question / cot / answer 这几个字段
    if (
        isinstance(raw, dict)
        and all(k in raw for k in ["question", "cot", "answer"])
        and all(isinstance(raw[k], list) for k in ["question", "cot", "answer"])
    ):
        n = len(raw["question"])
        data = []
        for i in range(n):
            q = raw["question"][i]
            cot_i = raw["cot"][i]
            ans = raw["answer"][i]

            # cot 可能是字符串，也可能已经是一个步列表
            if isinstance(cot_i, str):
                steps = [cot_i]
            elif isinstance(cot_i, list):
                steps = cot_i
            else:
                raise ValueError(
                    f"Unsupported cot element type: {type(cot_i)} at index {i}"
                )

            data.append(
                {
                    "question": q,
                    "steps": steps,
                    "answer": ans,
                }
            )

    # 情况2：顶层原本就是 list（比如原始Coconut格式）
    elif isinstance(raw, list):
        data = raw

    # 其他结构，一律报错
    else:
        raise ValueError(
            f"Unsupported data format in {path}, top-level type={type(raw)}, keys={list(raw.keys()) if isinstance(raw, dict) else None}"
        )

    # 截断到 max_size，并加 idx
    data = data[: int(max_size)]
    data = [{**d, "idx": idx} for idx, d in enumerate(data)]

    keys = data[0].keys()
    dataset = Dataset.from_dict({k: [d[k] for d in data] for k in keys})

    # ================= 修改 1: 使用 safe_map =================
    dataset = safe_map(
        dataset, tokenize_sample, remove_columns=list(dataset.features), num_proc=1
    )
    # =======================================================

    # verify
    d = data[0]
    complete = d["question"] + "\n" + "\n".join(d["steps"]) + "\n### " + d["answer"]
    complete_tokenized = tokenizer.encode(complete, add_special_tokens=True) + [
        tokenizer.eos_token_id
    ]
    
    return dataset


@dataclass
class MyExplainableCollator:

    tokenizer: PreTrainedTokenizerBase
    latent_id: Optional[int] = None
    label_pad_token_id: Optional[int] = -100

    def __call__(self, features, return_tensors=None):
        assert self.tokenizer.padding_side == "right"

        earliest_latent = [
            feature["input_ids"].index(self.latent_id)
            for feature in features
            if self.latent_id in feature["input_ids"]
        ]
        ### add exp list
        explainable_list = []
        max_explainable_length = -1
        max_question_length = -1
        max_length = -1
        
        for feature in features:
            if 'explainable_ids_list' in feature:
                explainable_flat = [item for sublist in feature["explainable_ids_list"] for item in sublist]
                explainable_list.append(explainable_flat)
                max_explainable_length = max(max_explainable_length, len(explainable_flat))
            
            if 'question_ids' in feature:
                max_question_length = max(max_question_length, len(feature['question_ids']))

            max_length = max(max_length, len(feature['input_ids']))


        if len(earliest_latent) > 0:  # if there are continuous thoughts in the sequence
            latest_earliest_latent = max(earliest_latent)
            for idx, feature in enumerate(features):
                if self.latent_id in feature["input_ids"]:
                    n_tok_pad = latest_earliest_latent - feature["input_ids"].index(
                        self.latent_id
                    )
                else:
                    n_tok_pad = 0
                feature["position_ids"] = [0] * n_tok_pad + list(
                    range(len(feature["input_ids"]))
                )
                feature["input_ids"] = [
                    self.tokenizer.pad_token_id
                ] * n_tok_pad + feature["input_ids"]
                if "labels" in feature:
                    feature["labels"] = [self.label_pad_token_id] * n_tok_pad + feature[
                        "labels"
                    ]
                feature["attention_mask"] = [0] * n_tok_pad + feature["attention_mask"]
                ### pad exp list
                if 'explainable_ids_list' in feature:
                    feature['explainable_ids_list'] = explainable_list[idx] + [0] * (max_explainable_length - len(explainable_list[idx]))
                
                if 'question_ids' in feature:
                    feature['question_ids'] = [
                        self.tokenizer.pad_token_id
                    ] * (max_question_length - len(feature['question_ids'])) + feature['question_ids']
                if 'question_attention_mask' in feature:
                    feature['question_attention_mask'] = [0] * (max_question_length - len(feature['question_attention_mask'])) + feature['question_attention_mask']
                
                if 'question_position_ids' in feature:
                    feature['question_position_ids'] = [0] * (max_question_length - len(feature['question_position_ids'])) + feature['question_position_ids']
        else:
            for idx, feature in enumerate(features):

                if 'explainable_ids_list' in feature:
                    feature['explainable_ids_list'] = explainable_list[idx] + [0] * (max_explainable_length - len(explainable_list[idx]))
                
                if 'question_ids' in feature:
                    feature['question_ids'] = [
                        self.tokenizer.pad_token_id
                    ] * (max_question_length - len(feature['question_ids'])) + feature['question_ids']
                if 'question_attention_mask' in feature:
                    feature['question_attention_mask'] = [0] * (max_question_length - len(feature['question_attention_mask'])) + feature['question_attention_mask']
                
                if 'question_position_ids' in feature:
                    feature['question_position_ids'] = [0] * (max_question_length - len(feature['question_position_ids'])) + feature['question_position_ids']
                # import pdb; pdb.set_trace()
                # print()
                feature["position_ids"] = [0] * (max_length-len(feature["position_ids"])) + list(
                    range(len(feature["input_ids"]))
                )
                feature["input_ids"] = [
                    self.tokenizer.pad_token_id
                ] * (max_length-len(feature["input_ids"])) + feature["input_ids"]
                if "labels" in feature:
                    feature["labels"] = [self.label_pad_token_id] * (max_length-len(feature["labels"])) + feature[
                        "labels"
                    ]
                feature["attention_mask"] = [0] * (max_length-len(feature["attention_mask"])) + feature["attention_mask"]
        return_tensors = "pt"

        label_name = "label" if "label" in features[0].keys() else "labels"

        non_label_position_features = [
            {
                k: v
                for k, v in feature.items()
                if k != label_name and k != "position_ids"
            }
            for feature in features
        ]

        # run through tokenizer without labels to ensure no side effects
        # import pdb; pdb.set_trace()
        batch = pad_without_fast_tokenizer_warning(
            self.tokenizer,
            non_label_position_features,
            padding=True,
            pad_to_multiple_of=None,
            return_tensors=return_tensors,
        )

        labels = (
            [feature[label_name] for feature in features]
            if label_name in features[0].keys()
            else None
        )
        if labels is not None and all(label is None for label in labels):
            labels = None
        position_ids = (
            [feature["position_ids"] for feature in features]
            if "position_ids" in features[0].keys()
            else None
        )
        # we have to pad the labels and position_ids manually as we cannot rely on `tokenizer.pad`

        if labels is not None:
            max_label_length = max(len(l) for l in labels)

            batch["labels"] = [
                label + [self.label_pad_token_id] * (max_label_length - len(label))
                for label in labels
            ]
            batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)

        if position_ids is not None:
            max_pos_length = max(len(l) for l in position_ids)

            batch["position_ids"] = [
                position_id + [0] * (max_pos_length - len(position_id))
                for position_id in position_ids
            ]
            batch["position_ids"] = torch.tensor(
                batch["position_ids"], dtype=torch.int64
            )

        return batch

@dataclass
class MyCollator:

    tokenizer: PreTrainedTokenizerBase
    latent_id: Optional[int] = None
    label_pad_token_id: Optional[int] = -100

    def __call__(self, features, return_tensors=None):

        assert self.tokenizer.padding_side == "right"

        """
        Pad the batch like this to maximize the reuse of kv cache.
        E.g.,
        
        xxxxxxxxxx<latent><latent>xxxxx--
        -----xxxxx<latent>xxxxxxxx-------
        ---xxxxxxx<latent><latent>xxxxxxx


        ("x" is word token, "-" is pad token)
        """

        earliest_latent = [
            feature["input_ids"].index(self.latent_id)
            for feature in features
            if self.latent_id in feature["input_ids"]
        ]

        if len(earliest_latent) > 0:  # if there are continuous thoughts in the sequence
            latest_earliest_latent = max(earliest_latent)
            for feature in features:
                if self.latent_id in feature["input_ids"]:
                    n_tok_pad = latest_earliest_latent - feature["input_ids"].index(
                        self.latent_id
                    )
                else:
                    n_tok_pad = 0
                feature["position_ids"] = [0] * n_tok_pad + list(
                    range(len(feature["input_ids"]))
                )
                feature["input_ids"] = [
                    self.tokenizer.pad_token_id
                ] * n_tok_pad + feature["input_ids"]
                if "labels" in feature:
                    feature["labels"] = [self.label_pad_token_id] * n_tok_pad + feature[
                        "labels"
                    ]
                feature["attention_mask"] = [0] * n_tok_pad + feature["attention_mask"]

        return_tensors = "pt"

        label_name = "label" if "label" in features[0].keys() else "labels"

        non_label_position_features = [
            {
                k: v
                for k, v in feature.items()
                if k != label_name and k != "position_ids"
            }
            for feature in features
        ]

        # run through tokenizer without labels to ensure no side effects
        batch = pad_without_fast_tokenizer_warning(
            self.tokenizer,
            non_label_position_features,
            padding=True,
            pad_to_multiple_of=None,
            return_tensors=return_tensors,
        )

        labels = (
            [feature[label_name] for feature in features]
            if label_name in features[0].keys()
            else None
        )
        if labels is not None and all(label is None for label in labels):
            labels = None
        position_ids = (
            [feature["position_ids"] for feature in features]
            if "position_ids" in features[0].keys()
            else None
        )
        # we have to pad the labels and position_ids manually as we cannot rely on `tokenizer.pad`

        if labels is not None:
            max_label_length = max(len(l) for l in labels)

            batch["labels"] = [
                label + [self.label_pad_token_id] * (max_label_length - len(label))
                for label in labels
            ]
            batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)

        if position_ids is not None:
            max_pos_length = max(len(l) for l in position_ids)

            batch["position_ids"] = [
                position_id + [0] * (max_pos_length - len(position_id))
                for position_id in position_ids
            ]
            batch["position_ids"] = torch.tensor(
                batch["position_ids"], dtype=torch.int64
            )

        return batch


def get_question_latent_dataset(
    scheduled_stage,
    base_dataset_valid,
    configs,
    start_id,
    latent_id,
    end_id,
    no_special_marker=False,
):

    def process_dataset(sample):

        if configs.pad_latent_to_max:
            max_latent_stage = configs.max_latent_stage
        else:
            max_latent_stage = min(
                configs.max_latent_stage, len(sample["steps_tokenized"])
            )

        k = min(max_latent_stage, scheduled_stage)

        k *= configs.c_thought

        tokens = (
            sample["question_tokenized"]
            + ([] if no_special_marker else [start_id])
            + [latent_id] * k
            + ([] if no_special_marker else [end_id])
        )

        return {
            "input_ids": tokens,
            "idx": sample["idx"],
            "attention_mask": [1] * len(tokens),
            "position_ids": list(range(len(tokens))),
        }

    # ================= 修改 2: 使用 safe_map =================
    return safe_map(
        base_dataset_valid, process_dataset, remove_columns=list(base_dataset_valid.features), num_proc=1
    )
    # =======================================================


def get_cot_latent_dataset(
    scheduled_stage,
    base_dataset,
    configs,
    start_id,
    latent_id,
    end_id,
    no_special_marker=False,
    shuffle=False,
):

    n_additional_tokens = 0 if no_special_marker else 2

    def process_dataset(sample):

        if (
            random.random() < configs.uniform_prob
        ):  # with some prob, randomly sample stage
            scheduled_stage_to_train = random.choice(
                list(range(len(sample["steps_tokenized"]) + 1))
            )
        else:
            scheduled_stage_to_train = scheduled_stage

        if scheduled_stage_to_train > configs.max_latent_stage:
            n_skip_steps = 10000  # skip all
            if configs.pad_latent_to_max:
                n_latent_tokens = configs.max_latent_stage
            else:
                n_latent_tokens = min(
                    len(sample["steps_tokenized"]), configs.max_latent_stage
                )

        else:
            n_skip_steps, n_latent_tokens = (
                scheduled_stage_to_train,
                scheduled_stage_to_train,
            )

        if configs.no_cot:
            n_skip_steps = 100  # skip all step
            n_latent_tokens = 0

        n_latent_tokens *= configs.c_thought

        tokens = (
            sample["question_tokenized"]
            + ([] if no_special_marker else [start_id])
            + [latent_id] * n_latent_tokens
            + ([] if no_special_marker else [end_id])
            + list(
                itertools.chain.from_iterable(sample["steps_tokenized"][n_skip_steps:])
            )
            + sample["answer_tokenized"]
        )

        return {
            "input_ids": tokens,
            "labels": [-100]
            * (
                len(sample["question_tokenized"])
                + n_latent_tokens
                + n_additional_tokens
            )
            + tokens[
                n_latent_tokens
                + n_additional_tokens
                + len(sample["question_tokenized"]) :
            ],
            "attention_mask": [1] * len(tokens),
            "idx": sample["idx"],
            "position_ids": list(range(len(tokens))),
        }

    # ================= 修改 3: 使用 safe_map =================
    processed_dataset = safe_map(
        base_dataset, process_dataset, remove_columns=list(base_dataset.features), num_proc=1
    )
    if shuffle:
        processed_dataset = processed_dataset.shuffle()
    
    return processed_dataset
    # =======================================================



def get_cot_with_explainable_latent_dataset(
    scheduled_stage,
    base_dataset,
    configs,
    start_id,
    latent_id,
    end_id,
    no_special_marker=False,
    shuffle=False,
):
    n_additional_tokens = 0 if no_special_marker else 2

    def process_dataset(sample):
        if (
            random.random() < configs.uniform_prob
        ):  # with some prob, randomly sample stage
            scheduled_stage_to_train = random.choice(
                list(range(len(sample["steps_tokenized"]) + 1))
            )
        else:
            scheduled_stage_to_train = scheduled_stage

        if hasattr(configs, 'progressive_train') and configs.progressive_train:
            n_latent_tokens = configs.max_latent_stage
            if 'llama' in configs.name:
                n_latent_tokens = 1
            
            if scheduled_stage_to_train > configs.max_latent_stage:
                n_skip_steps = 10000
            else:
                n_skip_steps = scheduled_stage_to_train
        else:
            if scheduled_stage_to_train > configs.max_latent_stage:
                n_skip_steps = 10000  # skip all
                if configs.pad_latent_to_max:
                    n_latent_tokens = configs.max_latent_stage
                else:
                    n_latent_tokens = min(
                        len(sample["steps_tokenized"]), configs.max_latent_stage
                    )

            else:
                n_skip_steps, n_latent_tokens = (
                    scheduled_stage_to_train,
                    scheduled_stage_to_train,
                )

        if configs.no_cot:
            n_skip_steps = 100  # skip all step
            n_latent_tokens = 0

        n_latent_tokens *= configs.c_thought
        
        if hasattr(configs, 'cot_behind') and configs.cot_behind:
            tokens = (
                sample["question_tokenized"]
                + list(
                    itertools.chain.from_iterable(sample["steps_tokenized"][n_skip_steps:])
                    # itertools.chain.from_iterable(sample["steps_tokenized"][999999999:])
                )
                + ([] if no_special_marker else [start_id])
                + [latent_id] * n_latent_tokens
                + ([] if no_special_marker else [end_id])
                + sample["answer_tokenized"]
            )
            if hasattr(configs, 'learnable_token') and configs.learnable_token:
                labels = (
                    [-100]
                * (
                    len(sample["question_tokenized"])
                )
                + tokens[
                    len(sample["question_tokenized"]) :
                ])
            else:
                # import pdb; pdb.set_trace()
                labels = (
                    [-100]
                * (
                    len(sample["question_tokenized"])
                )
                + tokens[
                    len(sample["question_tokenized"]) :
                ])
                labels = process_list(labels, start_id=start_id, end_id=end_id)
                # import pdb; pdb.set_trace()
                # print()
        else:
            tokens = (
                sample["question_tokenized"]
                + ([] if no_special_marker else [start_id])
                + [latent_id] * n_latent_tokens
                + ([] if no_special_marker else [end_id])
                + list(
                    itertools.chain.from_iterable(sample["steps_tokenized"][n_skip_steps:])
                )
                + sample["answer_tokenized"]
            )
            labels = (
                [-100]
            * (
                len(sample["question_tokenized"])
                + n_latent_tokens
                + n_additional_tokens
            )
            + tokens[
                n_latent_tokens
                + n_additional_tokens
                + len(sample["question_tokenized"]) :
            ])
        
        return {
            "explainable_ids_list": sample["steps_tokenized"][:] 
            if hasattr(configs, 'progressive_train') and configs.progressive_train 
            else sample["steps_tokenized"][:n_skip_steps],
            "input_ids": tokens,
            "labels": labels,
            "attention_mask": [1] * len(tokens),
            "idx": sample["idx"],
            "position_ids": list(range(len(tokens))),
        }

    # ================= 修改 4: 使用 safe_map =================
    processed_dataset = safe_map(
        base_dataset, process_dataset, remove_columns=list(base_dataset.features), num_proc=1
    )
    if shuffle:
        processed_dataset = processed_dataset.shuffle()
    
    return processed_dataset
    # =======================================================